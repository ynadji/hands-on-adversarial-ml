{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPEG Defense\n",
    "\n",
    "In this exercise, you will construct a defense against one of the `cleverhans` attacks by using JPEG compression on the adversarial examples before classification.\n",
    "\n",
    "## Notes\n",
    "* Don't be surprised if the accuracy isn't much better after your JPEG defense (+6% is good enough). Why do you think the JPEG defense is less effective with MNIST?\n",
    "* If you see a warning that there are no GPUs and CPUs are being used, ensure `nb_epochs=1`.\n",
    "* I recommend attacking the FGSM code in the notebook.\n",
    "* Useful TensorFlow functions are:\n",
    "  * `tf.image.encode_jpeg`\n",
    "  * `tf.image.decode_jpeg`\n",
    "  * `tf.image.convert_image_dtype`\n",
    "* Remember! MNIST images are in grayscale.\n",
    "* If you see errors like `ValueError: ... that name is already used` you may need to call `tf.reset_default_graph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE. This allows MNIST dataset loading without Internet access.\n",
    "import sys\n",
    "sys.path.append('../util')\n",
    "import dataset\n",
    "sys.modules['cleverhans.dataset'] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add conversion code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_compress(x, quality=75):\n",
    "    \"\"\"JPEG encode/decode `x` at the specified `quality`.\"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from cleverhans/cleverhans_tutorials/mnist_tutorial_tf.py\n",
    "\"\"\"\n",
    "This tutorial shows how to generate adversarial examples using FGSM\n",
    "and train a model using adversarial training with TensorFlow.\n",
    "It is very similar to mnist_tutorial_keras_tf.py, which does the same\n",
    "thing but with a dependence on keras.\n",
    "The original paper can be found at:\n",
    "https://arxiv.org/abs/1412.6572\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from cleverhans.loss import CrossEntropy\n",
    "from cleverhans.dataset import MNIST\n",
    "from cleverhans.utils_tf import model_eval\n",
    "from cleverhans.train import train\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.utils import AccuracyReport, set_log_level\n",
    "from cleverhans_tutorials.tutorial_models import ModelBasicCNN\n",
    "\n",
    "NB_EPOCHS = 6\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "CLEAN_TRAIN = True\n",
    "BACKPROP_THROUGH_ATTACK = False\n",
    "NB_FILTERS = 64\n",
    "\n",
    "\n",
    "def mnist_tutorial(train_start=0, train_end=60000, test_start=0,\n",
    "                   test_end=10000, nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                   learning_rate=LEARNING_RATE,\n",
    "                   clean_train=CLEAN_TRAIN,\n",
    "                   testing=False,\n",
    "                   backprop_through_attack=BACKPROP_THROUGH_ATTACK,\n",
    "                   nb_filters=NB_FILTERS, num_threads=None,\n",
    "                   label_smoothing=0.1):\n",
    "  \"\"\"\n",
    "  MNIST cleverhans tutorial\n",
    "  :param train_start: index of first training set example\n",
    "  :param train_end: index of last training set example\n",
    "  :param test_start: index of first test set example\n",
    "  :param test_end: index of last test set example\n",
    "  :param nb_epochs: number of epochs to train model\n",
    "  :param batch_size: size of training batches\n",
    "  :param learning_rate: learning rate for training\n",
    "  :param clean_train: perform normal training on clean examples only\n",
    "                      before performing adversarial training.\n",
    "  :param testing: if true, complete an AccuracyReport for unit tests\n",
    "                  to verify that performance is adequate\n",
    "  :param backprop_through_attack: If True, backprop through adversarial\n",
    "                                  example construction process during\n",
    "                                  adversarial training.\n",
    "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
    "  :return: an AccuracyReport object\n",
    "  \"\"\"\n",
    "\n",
    "  # Object used to keep track of (and return) key accuracies\n",
    "  report = AccuracyReport()\n",
    "\n",
    "  # Set TF random seed to improve reproducibility\n",
    "  tf.set_random_seed(1234)\n",
    "\n",
    "  # Set logging level to see debug information\n",
    "  set_log_level(logging.DEBUG)\n",
    "\n",
    "  # Create TF session\n",
    "  if num_threads:\n",
    "    config_args = dict(intra_op_parallelism_threads=1)\n",
    "  else:\n",
    "    config_args = {}\n",
    "  sess = tf.Session(config=tf.ConfigProto(**config_args))\n",
    "\n",
    "  # Get MNIST data\n",
    "  mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "  x_train, y_train = mnist.get_set('train')\n",
    "  x_test, y_test = mnist.get_set('test')\n",
    "\n",
    "  # Use Image Parameters\n",
    "  img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
    "  nb_classes = y_train.shape[1]\n",
    "\n",
    "  # Define input TF placeholder\n",
    "  x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
    "                                        nchannels))\n",
    "  y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "\n",
    "  # Train an MNIST model\n",
    "  train_params = {\n",
    "      'nb_epochs': nb_epochs,\n",
    "      'batch_size': batch_size,\n",
    "      'learning_rate': learning_rate\n",
    "  }\n",
    "  eval_params = {'batch_size': batch_size}\n",
    "  fgsm_params = {\n",
    "      'eps': 0.3,\n",
    "      'clip_min': 0.,\n",
    "      'clip_max': 1.\n",
    "  }\n",
    "  rng = np.random.RandomState([2017, 8, 30])\n",
    "\n",
    "  def do_eval(preds, x_set, y_set, report_key, report_text=None):\n",
    "    acc = model_eval(sess, x, y, preds, x_set, y_set, args=eval_params)\n",
    "    setattr(report, report_key, acc)\n",
    "    if report_text:\n",
    "      print('Test accuracy on %s examples: %0.4f' % (report_text, acc))\n",
    "\n",
    "  if clean_train:\n",
    "    model = ModelBasicCNN('model1', nb_classes, nb_filters)\n",
    "    preds = model.get_logits(x)\n",
    "    loss = CrossEntropy(model, smoothing=label_smoothing)\n",
    "\n",
    "    def evaluate():\n",
    "      do_eval(preds, x_test, y_test, 'clean_train_clean_eval', 'legitimate')\n",
    "\n",
    "    train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
    "          args=train_params, rng=rng, var_list=model.get_params())\n",
    "\n",
    "    # Calculate training error\n",
    "    if testing:\n",
    "      do_eval(preds, x_train, y_train, 'train_clean_train_clean_eval', None)\n",
    "\n",
    "    # Initialize the Fast Gradient Sign Method (FGSM) attack object and\n",
    "    # graph\n",
    "    fgsm = FastGradientMethod(model, sess=sess)\n",
    "    adv_x = fgsm.generate(x, **fgsm_params)\n",
    "    preds_adv = model.get_logits(adv_x)\n",
    "\n",
    "    # Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "    do_eval(preds_adv, x_test, y_test, 'clean_train_adv_eval', 'adversarial')\n",
    "\n",
    "    # Calculate training error\n",
    "    if testing:\n",
    "      do_eval(preds_adv, x_train, y_train, 'train_clean_train_adv_eval', None)\n",
    "    \n",
    "    # JPEG evaluation happens here!\n",
    "    for quality in range(10, 100, 5):\n",
    "        from functools import partial\n",
    "        fn = partial(jpeg_compress, quality=quality)\n",
    "        jpeg_adv_x = tf.map_fn(fn, tf.image.convert_image_dtype(adv_x, dtype=tf.uint8))\n",
    "        converted = tf.image.convert_image_dtype(jpeg_adv_x, dtype=tf.float32)\n",
    "        converted.set_shape((None, 28, 28, 1))\n",
    "        preds_jpeg_adv = model.get_logits(converted)\n",
    "        do_eval(preds_jpeg_adv, x_test, y_test, 'clean_train_adv_eval', 'jpeg (quality = %d)' % quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last loop above is where your `jpeg_compress` function is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "  from cleverhans_tutorials import check_installation\n",
    "  check_installation(__file__)\n",
    "\n",
    "  mnist_tutorial(nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                 learning_rate=LEARNING_RATE,\n",
    "                 clean_train=CLEAN_TRAIN,\n",
    "                 backprop_through_attack=BACKPROP_THROUGH_ATTACK,\n",
    "                 nb_filters=NB_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mnist_tutorial(nb_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
